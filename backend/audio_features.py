import os
import sys
import wave
import contextlib
import collections
import numpy as np
import pandas as pd
from pydub import AudioSegment
from dotenv import load_dotenv
from tempfile import NamedTemporaryFile
import webrtcvad
import requests

# ==== ç’°å¢ƒå¤‰æ•°ãƒ»APIåˆæœŸåŒ– ====
load_dotenv()
API_TYPE = os.getenv("OPENAI_API_TYPE", "openai")

BASE_API_URL = os.getenv("BASE_API_URL", "http://localhost:8000")
GPT_API_URL = BASE_API_URL+"/secure-gpt-chat"

if API_TYPE == "azure":
    from openai import AzureOpenAI
    client = AzureOpenAI(
        api_key=os.getenv("AZURE_OPENAI_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
    )
    MODEL = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
else:
    from openai import AzureOpenAI
    client = AzureOpenAI(
        api_key=os.getenv("AZURE_OPENAI_KEY"),
        api_version=os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview"),
        azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
)
MODEL = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

# ==== éŸ³å£°å‡¦ç† ====
def read_wave(path):
    with contextlib.closing(wave.open(path, 'rb')) as wf:
        channels = wf.getnchannels()
        assert channels == 1, "Mono audio only"
        width = wf.getsampwidth()
        assert width == 2
        rate = wf.getframerate()
        assert rate in (8000, 16000, 32000, 48000)
        frames = wf.readframes(wf.getnframes())
        return frames, rate

def frame_generator(frame_duration_ms, audio, sample_rate):
    n = int(sample_rate * frame_duration_ms / 1000.0) * 2
    offset = 0
    while offset + n < len(audio):
        yield audio[offset:offset + n]
        offset += n

def vad_collector(rate, frame_duration_ms, padding_duration_ms, vad, frames):
    num_padding = int(padding_duration_ms / frame_duration_ms)
    ring_buffer = collections.deque(maxlen=num_padding)
    triggered = False
    voiced = []
    start, end = 0, 0
    timestamp = 0.0
    duration = float(frame_duration_ms) / 1000

    for frame in frames:
        is_speech = vad.is_speech(frame, rate)
        if not triggered:
            ring_buffer.append((frame, timestamp))
            if len([f for f, t in ring_buffer if vad.is_speech(f, rate)]) > 0.9 * ring_buffer.maxlen:
                triggered = True
                start = ring_buffer[0][1]
                ring_buffer.clear()
        else:
            ring_buffer.append((frame, timestamp))
            if len([f for f, t in ring_buffer if not vad.is_speech(f, rate)]) > 0.9 * ring_buffer.maxlen:
                end = timestamp + duration
                voiced.append((start, end))
                triggered = False
                ring_buffer.clear()
        timestamp += duration

    if triggered:
        end = timestamp
        voiced.append((start, end))

    return voiced

def extract_vad_segments(wav_path, aggressiveness=3):
    audio, rate = read_wave(wav_path)
    vad = webrtcvad.Vad(aggressiveness)
    frames = list(frame_generator(30, audio, rate))
    segments = vad_collector(rate, 30, 300, vad, frames)
    df = pd.DataFrame(segments, columns=["start", "end"])
    df["duration"] = df["end"] - df["start"]
    df["speaker"] = "user"
    return df

def analyze_loudness(wav_path):
    audio = AudioSegment.from_wav(wav_path)
    return {
        "average_loudness_dBFS": round(audio.dBFS, 2),
        "max_loudness_dBFS": round(audio.max_dBFS, 2),
        "duration_sec": round(audio.duration_seconds, 2)
    }

def create_prompt(transcript_text, vad_df, loudness_info):
    avg_speech = round(vad_df["duration"].mean(), 2)
    max_pause = round(vad_df["start"].iloc[1:].sub(vad_df["end"].shift()[1:]).max(), 2)

    return f"""
ã“ã®é€šè©±ã‚’ä»¥ä¸‹ã®è¦³ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ï¼š

1. å£°ã®å¤§ãã•ã‚„ãƒˆãƒ¼ãƒ³ï¼ˆãƒ©ã‚¦ãƒ‰ãƒã‚¹ï¼‰
2. æ²ˆé»™ã®å–ã‚Šæ–¹ã‚„ã€Œé–“ã€ã®é©åˆ‡ã•
3. ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°ã‚„è¦ç‚¹ã®ä¼ãˆæ–¹
4. ãƒ©ãƒãƒ¼ãƒ«å½¢æˆã‚„è‡ªç„¶ãªä¼šè©±ã‹ã©ã†ã‹
5. ç·åˆã‚¹ã‚³ã‚¢ï¼ˆ5ç‚¹æº€ç‚¹ï¼‰ã¨ãã®ç†ç”±

# æ›¸ãèµ·ã“ã—
{transcript_text}

# éŸ³å£°ç‰¹å¾´é‡
- å¹³å‡ç™ºè©±æ™‚é–“: {avg_speech} ç§’
- æœ€å¤§æ²ˆé»™æ™‚é–“: {max_pause} ç§’
- å¹³å‡éŸ³é‡: {loudness_info['average_loudness_dBFS']} dBFS
- æœ€å¤§éŸ³é‡: {loudness_info['max_loudness_dBFS']} dBFS
- é€šè©±å…¨ä½“é•·: {loudness_info['duration_sec']} ç§’
"""

def evaluate_with_gpt(prompt_text):
    """GPTã§ãƒ†ã‚­ã‚¹ãƒˆã‚’è©•ä¾¡"""
    try:
        # âœ… 'text' â†’ 'user_message' ã«ä¿®æ­£
        response = requests.post(
            GPT_API_URL, 
            json={"user_message": prompt_text},  # â† ä¿®æ­£
            timeout=30
        )
        response.raise_for_status()
        return response.json().get("reply", "è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {e}"

def extract_audio_features_from_uploaded_file(uploaded_file_or_path):
    if isinstance(uploaded_file_or_path, str) and os.path.isfile(uploaded_file_or_path):
        wav_path = uploaded_file_or_path
    else:
        with NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(uploaded_file_or_path.read())
            wav_path = tmp.name

    df = extract_vad_segments(wav_path)
    loudness = analyze_loudness(wav_path)

    return {
        "å¹³å‡ç™ºè©±æ™‚é–“ï¼ˆç§’ï¼‰": round(df["duration"].mean(), 2),
        "æœ€å¤§æ²ˆé»™æ™‚é–“ï¼ˆç§’ï¼‰": round(df["start"].iloc[1:].sub(df["end"].shift()[1:]).max(), 2),
        "å¹³å‡éŸ³é‡ï¼ˆdBFSï¼‰": loudness["average_loudness_dBFS"],
        "æœ€å¤§éŸ³é‡ï¼ˆdBFSï¼‰": loudness["max_loudness_dBFS"],
        "é€šè©±æ™‚é–“ï¼ˆç§’ï¼‰": loudness["duration_sec"]
    }

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python audio_features.py <wav_path> <transcript_path>")
        sys.exit(1)

    wav_path = sys.argv[1]
    txt_path = sys.argv[2]

    df = extract_vad_segments(wav_path)
    loudness = analyze_loudness(wav_path)
    transcript = open(txt_path, encoding="utf-8").read()
    prompt = create_prompt(transcript, df, loudness)
    gpt_result = evaluate_with_gpt(prompt)

    print("\n==== ğŸ§  GPTè©•ä¾¡çµæœ ====")
    print(gpt_result)
